{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark NLP Application\n",
    "## Working version for AWS EC2 cluster deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark application\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop() # use to stop the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !!!Assure that all libraries are installed on all computers in the cluster!!!\n",
    "\n",
    "# General\n",
    "import json\n",
    "import re, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emot\n",
    "from collections import Counter\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Visual\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# AWS\n",
    "import boto3\n",
    "\n",
    "# Data preprocessing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType\n",
    "\n",
    "# Machine Learning\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import NaiveBayes, SVMWithSGD, SVMModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors as MLLibVectors\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In you don't have these modules dowloaded.\n",
    "\n",
    "#nltk.downloader.download('wordnet')\n",
    "#nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you correctly configured Spark cluster with s3a file system credentials you can skipen commented lines below.\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('BUCKET_NAME')\n",
    "#aws_secret_access_key = ''\n",
    "#aws_access_key_id = ''\n",
    "#sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", aws_access_key_id)\n",
    "#sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", aws_secret_access_key)\n",
    "object_list = [k for k in bucket.objects.all() ]\n",
    "key_list = [k.key for k in bucket.objects.all()]\n",
    "key_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paths = ['s3a://'+o.bucket_name+'/'+ o.key for o in object_list]\n",
    "path_reviews = paths[0]\n",
    "data = sqlContext.read.json(path_reviews)\n",
    "data.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata = paths[1]\n",
    "metadata = spark.read.json(path_metadata)\n",
    "metadata.createOrReplaceTempView(\"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = data.count()\n",
    "n_distinct = data.distinct().count()\n",
    "users = data.select([c for c in data.columns if c == 'reviewerID']).distinct().count()\n",
    "books = data.select([c for c in data.columns if c == 'asin']).distinct().count()\n",
    "\n",
    "print('Number of observations: {0}'.format(n))\n",
    "print('Number of unique observations: {0}'.format(n_distinct))\n",
    "print('Number of duplicates: {0}'.format(n - n_distinct))\n",
    "print('Number of users: {0}'.format(users))\n",
    "print('Number of books: {0}'.format(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_data = sample_data.dropDuplicates(subset = [c for c in sample_data.columns if c not in ['asin', 'reviewerID']])\n",
    "sample_data.createOrReplaceTempView(\"sample_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom function to extract emojis from text.\n",
    "\n",
    "def emoji(string):\n",
    "    start = ' '.join([i for i in string.split(' ') if not any(a in i for a in ['oo', 'OO', 'xp'])])\n",
    "    first = emot.emoticons(start)\n",
    "    second = set([i['value'] for i in first if i and i != ')'])\n",
    "    return list(second)\n",
    "\n",
    "emoji_detector = udf(emoji)\n",
    "data = data.withColumn('Emoji', emoji_detector(col('reviewText')))\n",
    "data.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation of short summary with the comment\n",
    "sqlTransform = SQLTransformer(statement=\"SELECT *, concat(summary, ' ', reviewText) as text, concat(asin, ' ', reviewerID) as ID FROM __THIS__\")\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=\"[^\\\\p{L}]\")\n",
    "\n",
    "# Remving stop words from text\n",
    "stopremover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"stop_tokens\")\n",
    "\n",
    "# Vectorization of the text\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='rawFeatures')\n",
    "\n",
    "# Calcualtion of inverted document frequency\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Building feature extraction pipeline\n",
    "data_preproc = Pipeline(stages=[sqlTransform, tokenizer, stopremover, count_vec, idf])\n",
    "\n",
    "# Extracting the features\n",
    "cleaner = data_preproc.fit(data)\n",
    "data = cleaner.transform(data)\n",
    "data.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER sentiment analyzer application\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sent(scoring):\n",
    "    if scoring['compound'] > 0.5:\n",
    "        a = 'positive'\n",
    "    elif scoring['compound'] < 0.5 and scoring['compound'] > -0.5:\n",
    "        a = 'neutral'\n",
    "    else:\n",
    "        a = 'negative'\n",
    "    return a\n",
    "\n",
    "# Lemmatization of words\n",
    "\n",
    "def lemma(x):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return [wordnet_lemmatizer.lemmatize(t) for t in x]\n",
    "\n",
    "# Additional features\n",
    "\n",
    "sentiment = udf(lambda x: sent(analyzer.polarity_scores(x)), StringType())\n",
    "votes = udf(lambda h: None if h[1] == 0 else h[1], IntegerType())\n",
    "helpful = udf(lambda h: h[0], IntegerType())\n",
    "lemmatize = udf(lemma, ArrayType(StringType(), True))\n",
    "score = udf(lambda x: analyzer.polarity_scores(x)['compound'], FloatType())\n",
    "\n",
    "data = data.withColumn('ReviewDate', to_date(from_unixtime(col(\"unixReviewTime\"), format='yyyy-MM-dd'))) \\\n",
    "                     .withColumn('HelpfulVotes', helpful(col(\"helpful\"))) \\\n",
    "                     .withColumn('TotalVotes', votes(col(\"helpful\"))) \\\n",
    "                     .withColumn('RawReviewLength', length(col(\"reviewText\"))) \\\n",
    "                     .withColumn('TokensLength', size(col(\"tokens\"))) \\\n",
    "                     .withColumn('StopTokensLength', size(col(\"stop_tokens\"))) \\\n",
    "                     .withColumn('Lemmatized', lemmatize(col(\"stop_tokens\"))) \\\n",
    "                     .withColumn('Sentiment', sentiment(col(\"reviewText\"))) \\\n",
    "                     .withColumn('Compound_Score', score(col(\"reviewText\"))) \\\n",
    "                     .select('ID', 'asin', 'reviewerID', 'ReviewDate', 'TotalVotes', 'HelpfulVotes', 'overall', 'TokensLength', 'StopTokensLength',\n",
    "                             'RawReviewLength', 'text', 'stop_tokens', 'Lemmatized', 'Sentiment', 'Compound_Score', 'features')\n",
    "\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "#data.cache() # Cache the dataframe if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data consolidation and outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fraction of missing values in columns\n",
    "\n",
    "data.agg(*[(1 - (count(c) / count('*'))).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing missing values and consolidating the dataset with metadata\n",
    "\n",
    "data = data.dropna() \\\n",
    "           .withColumn('Helpfulness', col(\"HelpfulVotes\")/col('TotalVotes'))\n",
    "data = data.filter(data.Helpfulness <= 1)\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "\n",
    "metadata = spark.sql(\"\"\"select asin, title, price, salesRank['Books'] as SalesRank\n",
    "                            from metadata\n",
    "                            where asin is not null and price is not null\n",
    "                                  and salesRank['Books'] is not null\"\"\")\n",
    "metadata.createOrReplaceTempView(\"working_meta\")\n",
    "\n",
    "data_set = spark.sql(\"\"\"select * from data a\n",
    "                        left join metadata b\n",
    "                        on a.asin = b.asin\n",
    "                        where b.asin is not null\"\"\")\n",
    "data_set.createOrReplaceTempView(\"data_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting from RDD to Pandas DataFrame\n",
    "\n",
    "numerical_columns = ['TotalVotes', 'HelpfulVotes', 'overall', 'TokensLength', 'StopTokensLength',\n",
    "                    'RawReviewLength', 'Compound_Score', 'Helpfulness', 'price', 'SalesRank', 'Sentiment']\n",
    "pdData = data_set.select(numerical_columns).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot for sentiments groups\n",
    "\n",
    "plt.clf()\n",
    "sns.set(style=\"darkgrid\")\n",
    "hist = sns.countplot(x=\"Sentiment\", data=pdData, palette=\"Greens_d\")\n",
    "total = float(len(pdData))\n",
    "for p in hist.patches:\n",
    "    height = p.get_height()\n",
    "    hist.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}'.format(height),\n",
    "            ha=\"center\") \n",
    "#display(hist.figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot for price\n",
    "\n",
    "plt.clf()\n",
    "sns.distplot(pdData.price, color = 'lightseagreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sns.set(style=\"ticks\")\n",
    "len_cols =  [i for i in pdData.columns if 'Length' in i]\n",
    "d = {}\n",
    "for c in len_cols:\n",
    "    g = sns.jointplot(x=pdData[c], y=pdData[\"Helpfulness\"], kind='hex', color=\"#4CB391\")\n",
    "    d[c] = g.fig\n",
    "\n",
    "d[len_cols[2]]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation and distribution for 'price' and 'helpfulness'\n",
    "\n",
    "plt.clf()\n",
    "g = sns.JointGrid(x=\"Helpfulness\", y=\"price\", data=pdData) \n",
    "g.plot_joint(sns.regplot, order=2)\n",
    "g.plot_marginals(sns.distplot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonal correlation matrix for numerival features\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "corr = pdData[num_cols].corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "cmap = sns.diverging_palette(200, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.5, center=0,\n",
    "            square=True, linewidths=.65, cbar_kws={\"shrink\": .5})\n",
    "ax.set_title('Diagonal correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots\n",
    "\n",
    "plt.clf()\n",
    "g = sns.FacetGrid(pdData, col=\"Sentiment\")\n",
    "g.map(plt.scatter, \"TotalVotes\", \"StopTokensLength\", alpha=.5)\n",
    "g.add_legend()\n",
    "plt.figure(figsize=(15,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_sentiment = pdData.groupby('Sentiment').agg({\"price\": \"max\", \"overall\": \"mean\"})\n",
    "metrics_by_sentiment.columns = ['Average Score', 'Maximum Price']\n",
    "metrics_by_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tukey outlier rule implementation\n",
    "\n",
    "cols = ['price', 'RawReviewLength']\n",
    "bounds = {}\n",
    "\n",
    "for col in cols:\n",
    "    quantiles = data_set.approxQuantile(col, [0.25, 0.75], 0)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [\n",
    "        quantiles[0] - 1.5 * IQR, \n",
    "        quantiles[1] + 1.5 * IQR\n",
    "]\n",
    "    \n",
    "print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "\n",
    "outliers = data_set.select(*['ID'] + [\n",
    "    (\n",
    "        (data_set[c] > 0) & (data_set[c] < bounds[c][1])\n",
    "    ).alias(c + '_out') for c in cols\n",
    "])\n",
    "#outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = data_set.join(outliers, on='ID')\n",
    "df_out = df_outliers.filter('price_out').filter('RawReviewLength_out').filter(df_outliers.TotalVotes < 200) \\\n",
    "                         .drop('price_out').drop('RawReviewLength_out')\n",
    "#display(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdData_out = df_out.select(num_cols).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(15,9))\n",
    "sns.violinplot(x=\"Sentiment\", y=\"RawReviewLength\", data=pdData_out, palette=\"muted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "sns.set(style=\"darkgrid\")\n",
    "hist = sns.countplot(x=\"Sentiment\", data=pdData_out, palette=\"Greens_d\")\n",
    "total = float(len(pdData_out))\n",
    "for p in hist.patches:\n",
    "    height = p.get_height()\n",
    "    hist.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}'.format(height),\n",
    "            ha=\"center\") \n",
    "#display(hist.figure)\n",
    "plt.figure(figsize=(13,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.set(style=\"ticks\")\n",
    "g = sns.jointplot(x=pdData_out['StopTokensLength'], y=pdData_out[\"Compound_Score\"], kind='hex', color=\"maroon\", stat_func=spearmanr)\n",
    "g.fig.suptitle('StopTokensLength vs. RawReviewLength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud\n",
    "\n",
    "def most_common_words(set, column, n = 100):\n",
    "    data_pd = set.select(column).toPandas()\n",
    "    reviews = data_pd[column].tolist()\n",
    "    out = []\n",
    "    for i in range(0, len(reviews)):\n",
    "        out.extend([e.encode('utf-8').strip() for e in reviews[i] if len(e.encode('utf-8').strip()) > 2])\n",
    "    out = Counter(out).most_common(n)\n",
    "    toPD = pd.DataFrame(out, columns = ['Words', ' Count']).set_index('Words')\n",
    "    d = {}\n",
    "    for i in range(0, n):\n",
    "        d[list(toPD.index)[i]] = toPD.iloc[:,0][i]\n",
    "    return d\n",
    "\n",
    "def show_wordcloud(set, backgroud = \"white\"):\n",
    "    wc = WordCloud(background_color = backgroud, max_words=2000,\n",
    "               max_font_size=40, random_state=42)\n",
    "    wc.generate_from_frequencies(set)\n",
    "    wcd = plt.figure()\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    display(wcd)\n",
    "\n",
    "top_100 = most_common_words(data_set, 'Lemmatized')\n",
    "show_wordcloud(top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.select('ID', 'title', 'price', 'SalesRank', 'ReviewDate', 'overall', 'Lemmatized', 'TotalVotes', 'HelpfulVotes',\n",
    "                           'Helpfulness', 'RawReviewLength', 'StopTokensLength', 'features', 'Compound_Score', 'Sentiment') \\\n",
    "                   .filter(data_set.Sentiment != 'neutral')\n",
    "\n",
    "df_out.createOrReplaceTempView(\"df_out\")\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\")\n",
    "fitted_data = indexer.fit(df_out)\n",
    "indexed_data = fitted_data.transform(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display labeled records\n",
    "\n",
    "indexed_data.select('Sentiment', 'label').show(n = 10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling for positive records\n",
    "\n",
    "stratified_data = indexed_data.sampleBy('label', fractions={0: 269356./1832111, 1: 1.0})\n",
    "stratified_data.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_data.show(n = 10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = stratified_data.select(['label', 'features']).randomSplit([0.8,0.2],seed=123)\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_float = udf(lambda x: x['label'], FloatType())\n",
    "label = stratified_data.rdd.map(lambda x: to_float(x))\n",
    "features = stratified_data.rdd.map(lambda x: (x['features'], ))\n",
    "scaler = StandardScaler().fit(features).transform(features)\n",
    "data1 = label.zip(scaler).toDF(['label', 'features'])\n",
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "\n",
    "nb = NaiveBayes(smoothing=3.0, modelType=\"multinomial\")\n",
    "\n",
    "# Training\n",
    "model = nb.fit(train)\n",
    "\n",
    "# Predicting\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Checking the quality of the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (prediction, label) pairs\n",
    "predictionAndLabel = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "# Generate confusion matrix\n",
    "metrics = MulticlassMetrics(predictionAndLabel)\n",
    "print(metrics.confusionMatrix())\n",
    "confMatrx = metrics.confusionMatrix().toArray().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.heatmap(confMatrx, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "#display(ax.figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch on smoothing parameter\n",
    "\n",
    "smoothing = np.arange(0.0, 1.0, 0.1).tolist()\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, smoothing).build()\n",
    "cvEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(), numFolds=5)\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "cvPredictions = cvModel.transform(test)\n",
    "cvPredictions.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set accuracy = \" + str(cvEvaluator.evaluate(cvPredictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvPredictions_val = cvPredictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "metrics_cv = MulticlassMetrics(cvPredictions_val)\n",
    "print(metrics_cv.confusionMatrix())\n",
    "confMatrx_cv = metrics_cv.confusionMatrix().toArray().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(13,10))\n",
    "sns.heatmap(confMatrx_cv, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "#display(ax.figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Grams model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ngram column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "ngram = NGram(inputCol = 'Lemmatized', outputCol = 'ngram', n = n)\n",
    "add_ngram = ngram.transform(stratified_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorizer and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_ngram = CountVectorizer(inputCol='ngram', outputCol='tf_ngram')\n",
    "cvModel_ngram = cv_ngram.fit(add_ngram)\n",
    "cv_df_ngram = cvModel_ngram.transform(add_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_ngram = IDF().setInputCol('tf_ngram').setOutputCol('tfidf_ngram')\n",
    "tfidfModel_ngram = idf_ngram.fit(cv_df_ngram)\n",
    "tfidf_df_ngram = tfidfModel_ngram.transform(cv_df_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training & testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_ngram = tfidf_df_ngram.select(['tfidf_ngram', 'label']).randomSplit([0.8,0.2],seed=123)\n",
    "train_ngram = splits_ngram[0].cache()\n",
    "test_ngram = splits_ngram[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert feature matrix to LabeledPoint vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))\n",
    "test_lb_ngram = train_ngram.rdd.map(lambda row: LabeledPoint(row[1], MLLibVectors.fromML(row[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit SVM model of only trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to tune this model\n",
    "\n",
    "numIterations = 50\n",
    "regParam = 0.3\n",
    "svm = SVMWithSGD.train(train_lb_ngram, numIterations, regParam=regParam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract top 20 trigrams based on weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreAndLabels_test = test_lb_ngram.map(lambda x: (float(svm.predict(x.features)), x.label))\n",
    "score_label_test = spark.createDataFrame(scoreAndLabels_test, [\"prediction\", \"label\"])\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "svm_f1 = f1_eval.evaluate(score_label_test)\n",
    "print(\"F1 score: %.4f\" % svm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ngram.select('ngram').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(train)\n",
    "\n",
    "ll = model.logLikelihood(train)\n",
    "lp = model.logPerplexity(train)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics()\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(test)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(inputCol='Lemmatized',outputCol='rawFeatures')\n",
    "count_vec_model = count_vec.fit(stratified_data)\n",
    "\n",
    "vocab = count_vec_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_words = topics.rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: \", idx)\n",
    "    print(\"----------\")\n",
    "    for word in topic:\n",
    "        print(word)\n",
    "    print(\"----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "Working File",
  "notebookId": 580639912023228
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
